{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PFQUEWFEu-bD"
      },
      "outputs": [],
      "source": [
        "# First run will required runtime restart.\n",
        "# Have to restart and run again.\n",
        "\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install ultralytics==8.0.196\n",
        "!pip install ffmpeg-python\n",
        "!pip install roboflow\n",
        "!pip install py7zr\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im3jqysKg7vQ",
        "outputId": "989109b8-66ac-4977-84d7-28af4d1537e0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import py7zr\n",
        "import os\n",
        "\n",
        "def download_model_via_api(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        print(f\"Failed to download model. Status code: {response.status_code}, Response: {response.text}\")\n",
        "        raise Exception(\"Failed to download model weights.\")\n",
        "\n",
        "# Adjusted for direct download from GitHub\n",
        "model_weights_url = 'https://github.com/leekaize/COS30018/raw/main/02_FinalModel/FinalMulti.7z'\n",
        "model_weights_path = '/content/FinalMulti.7z'\n",
        "\n",
        "# Adjusted for direct download from GitHub\n",
        "or_model_weights_url = 'https://github.com/leekaize/COS30018/raw/main/ObjectRecognitionModel/latest_prod_recog_best.7z'\n",
        "or_model_weights_path = '/content/ProductRecognition.7z'\n",
        "\n",
        "# Adjusted for direct download from GitHub\n",
        "reference_plan_img_url = 'https://github.com/leekaize/COS30018/raw/main/04_Planogram/ReferencePlanogramPerspective.jpg'\n",
        "reference_plan_img_path = '/content/ReferencePlanogramImagePerspective.jpg'\n",
        "\n",
        "# Adjusted for direct download from GitHub\n",
        "reference_plan_label_url = 'https://github.com/leekaize/COS30018/raw/main/04_Planogram/ReferencePlanogramLabel.txt'\n",
        "reference_plan_label_path = '/content/reference_plan_label.txt'\n",
        "\n",
        "# Adjusted for direct download from GitHub\n",
        "planogram_functions_url = 'https://github.com/leekaize/COS30018/raw/main/04_Planogram/planogram.py'\n",
        "planogram_functions_path = '/content/planogram_function.py'\n",
        "\n",
        "# Download the missing and imprecise model weight\n",
        "download_model_via_api(model_weights_url, model_weights_path)\n",
        "\n",
        "# Download the object recognition model weight\n",
        "download_model_via_api(or_model_weights_url, or_model_weights_path)\n",
        "\n",
        "# Download the reference planogram image's perspective\n",
        "download_model_via_api(reference_plan_img_url, reference_plan_img_path)\n",
        "\n",
        "# Download the reference planogram bounding box labels\n",
        "download_model_via_api(reference_plan_label_url, reference_plan_label_path)\n",
        "\n",
        "# Download the .py file for planogram functions\n",
        "download_model_via_api(planogram_functions_url, planogram_functions_path)\n",
        "\n",
        "# Extract the 7zip files\n",
        "with py7zr.SevenZipFile(model_weights_path, 'r') as archive:\n",
        "    archive.extractall('/content/model_weights')\n",
        "with py7zr.SevenZipFile(or_model_weights_path, 'r') as archive:\n",
        "    archive.extractall('/content/or_model_weights')\n",
        "\n",
        "# Check the extracted files\n",
        "print(os.listdir('/content/model_weights'))\n",
        "print(os.listdir('/content/or_model_weights'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f11AGJa1waxE",
        "outputId": "f903e039-33c4-4780-ac6b-1c354aadbe69"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import ultralytics\n",
        "import streamlit as st\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageOps\n",
        "import cv2\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import os\n",
        "import ffmpeg\n",
        "from roboflow import Roboflow\n",
        "from planogram_function import *\n",
        "\n",
        "# Load the YOLO model\n",
        "# Missing and Imprecise\n",
        "model = YOLO('/content/model_weights/FinalMulti.pt')\n",
        "# Object recognition\n",
        "recognition_model = YOLO('/content/or_model_weights/latest_prod_recog_best.pt')\n",
        "    \n",
        "rf = Roboflow(api_key=\"Qcpny2sVf8g33aLNl9iL\")\n",
        "project = rf.workspace(\"cos30018\").project(\"incrementallearning\")\n",
        "\n",
        "# load the reference planogram bounding box locations\n",
        "reference_plan_path = \"/content/reference_plan_label.txt\"\n",
        "data  = read_reference_file(reference_plan_path)\n",
        "\n",
        "reference_img = cv2.imread('/content/ReferencePlanogramImagePerspective.jpg')\n",
        "\n",
        "# Define the new width and height for reference image(for downsampling)\n",
        "reference_new_width = int(reference_img.shape[1] / 2)\n",
        "reference_new_height = int(reference_img.shape[0] / 2)\n",
        "\n",
        "# Downsample the image using resize function\n",
        "downsampled_reference_image = cv2.resize(reference_img, (reference_new_width, reference_new_height))\n",
        "\n",
        "# Convert images to grayscale (to optimize sift detection)\n",
        "reference_image_gray = cv2.cvtColor(downsampled_reference_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detects and compute descriptors using sift\n",
        "sift = cv2.SIFT_create()\n",
        "keypoints2, descriptors2 = sift.detectAndCompute(reference_image_gray, None)\n",
        "\n",
        "def upload_to_roboflow(image_path, annotation_path, project):\n",
        "    try:\n",
        "        response = project.upload(image_path=image_path, annotation_path=annotation_path, annotation_format=\"yolo\")\n",
        "        print(f\"Uploaded {image_path} and {annotation_path} successfully.\")\n",
        "        print(\"Upload response:\", response)\n",
        "        # Delete files after successful upload\n",
        "        os.remove(image_path)\n",
        "        os.remove(annotation_path)\n",
        "        print(f\"Deleted {image_path} and {annotation_path} after upload.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to upload {image_path} and {annotation_path}: {e}\")\n",
        "\n",
        "def detection_page():\n",
        "    st.title(\"Detection\")\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        uploaded_file = st.file_uploader(\"Upload an image or video...\", type=[\"png\", \"jpg\", \"jpeg\", \"mp4\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Reset session state for valid_detections on new file upload\n",
        "        st.session_state.valid_detections = []\n",
        "\n",
        "        file_type = uploaded_file.type.split('/')[0]\n",
        "        if file_type == 'image':\n",
        "            with col2:\n",
        "                image = Image.open(uploaded_file).convert('RGB')\n",
        "                image = ImageOps.exif_transpose(image)\n",
        "                results = model.predict(image, conf=0.5)\n",
        "                np_array = results[0].plot()\n",
        "                np_array = cv2.cvtColor(np_array, cv2.COLOR_BGR2RGB)\n",
        "                predicted_image = Image.fromarray(np_array, 'RGB')\n",
        "                st.image(predicted_image, caption='Processed Image with Detections.', use_column_width=True)\n",
        "        elif file_type == 'video':\n",
        "            with col2:\n",
        "                tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n",
        "                tfile.write(uploaded_file.read())\n",
        "                results = model.predict(tfile.name, save=True, project=\"videos\")\n",
        "                processed_video_path = results[0].save_dir + results[0].path\n",
        "                processed_video_path = processed_video_path.replace(\"/tmp/\", \"/\")\n",
        "                temp_video_path = processed_video_path.replace(\".mp4\", \".avi\")\n",
        "\n",
        "                (\n",
        "                    ffmpeg\n",
        "                    .input(temp_video_path)\n",
        "                    .output(processed_video_path, vcodec='libx264', crf=28, preset='ultrafast', movflags='faststart', acodec='aac', strict='experimental')\n",
        "                    .run()\n",
        "                )\n",
        "\n",
        "                st.video(processed_video_path)\n",
        "\n",
        "def planogram_page(): \n",
        "    st.title(\"Planogram Compliance\")\n",
        "    col1, col2 = st.columns(2)\n",
        "    \n",
        "    with col1:\n",
        "        uploaded_file = st.file_uploader(\"Upload an image or video...\", type=[\"png\", \"jpg\", \"jpeg\", \"mp4\"])\n",
        "    \n",
        "    if uploaded_file is not None:\n",
        "        # Reset session state for valid_detections on new file upload\n",
        "        st.session_state.valid_detections = []\n",
        "\n",
        "        file_type = uploaded_file.type.split('/')[0]\n",
        "        if file_type == 'image':\n",
        "            with col2:\n",
        "                image = Image.open(uploaded_file).convert('RGB')\n",
        "                image = ImageOps.exif_transpose(image)\n",
        "                image = np.array(image)\n",
        "                # Downsample the image to speed up keypoints detection\n",
        "                image_new_width = int(image.shape[1]/ 2)\n",
        "                image_new_height = int(image.shape[0]/2 )\n",
        "                downsampled_image = cv2.resize(image, (image_new_width, image_new_height))\n",
        "\n",
        "                # Convert images to grayscale (to optimize sift detection)\n",
        "                downsampled_image_gray = cv2.cvtColor(downsampled_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                # Detect keypoints and compute descriptors using sift\n",
        "                keypoints1, descriptors1 = sift.detectAndCompute(downsampled_image_gray, None)\n",
        "                \n",
        "                # Match keypoints\n",
        "                matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "                matches = matcher.match(descriptors1, descriptors2)\n",
        "                \n",
        "                #store the minimum distance for best fit\n",
        "                min_dist = float('inf')\n",
        "                # Iterate over all matches to find the minimum distance\n",
        "                for m in matches:\n",
        "                    if m.distance < min_dist:\n",
        "                        min_dist = m.distance\n",
        "                \n",
        "                # Filter weak matches\n",
        "                good_matches = []\n",
        "                distance_threshold = 2  # Threshold for point to be considered inliers\n",
        "                for m in matches:\n",
        "                    if m.distance < distance_threshold * min_dist:\n",
        "                        good_matches.append(m)\n",
        "                        \n",
        "                # Extract matched keypoints using the good_matches(source and destination points)\n",
        "                src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "                dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "                \n",
        "                # Estimate homography using RANSAC with ransac reprojection threshold of 10 to limit distortion\n",
        "                homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransacReprojThreshold=10)\n",
        "\n",
        "                # Apply perspective transformation to source image and convert to RGB format\n",
        "                transformed_img = cv2.warpPerspective(downsampled_image, homography, (downsampled_reference_image.shape[1], downsampled_reference_image.shape[0]))\n",
        "                transformed_img_rgb = cv2.cvtColor(transformed_img, cv2.COLOR_BGR2RGB)\n",
        "                        \n",
        "                # Perform detection\n",
        "                results = recognition_model.predict(transformed_img_rgb, conf=0.4)\n",
        "                \n",
        "                # read the predicted result from object recognition into array as tuples(classID, xmin, ymin, xmax, ymax)(normalized coordinates)\n",
        "                objects_predicted_tuples = []\n",
        "                # attach classID to their respective bouunding boxes\n",
        "                for result in results:\n",
        "                    for box in result.boxes:\n",
        "                        objects_predicted_tuples.append((int(box.cls), float(box.xyxyn[0][0]), float(box.xyxyn[0][1]), float(box.xyxyn[0][2]), float(box.xyxyn[0][3])))\n",
        "                \n",
        "                # convert tuples into arrays\n",
        "                object_predicted_array = np.array(objects_predicted_tuples)\n",
        "                \n",
        "                # Extract matching bounding boxes\n",
        "                bounding_boxes = []\n",
        "                for reference in data:\n",
        "                    checkMatch = 0\n",
        "                    for prediction in object_predicted_array:\n",
        "                        if prediction[0] == reference[0]:\n",
        "                            if compute_iou(prediction[1:],reference[1:]) > 0.5:\n",
        "                                checkMatch = 1\n",
        "                    if checkMatch == 0:\n",
        "                        bounding_boxes.append(reference)\n",
        "                \n",
        "                enlarge_bbox = []\n",
        "                for bbox in bounding_boxes:\n",
        "                    enlarge_bbox.append(convert_normalized_to_pixel(bbox[1:], transformed_img_rgb.shape[0], transformed_img_rgb.shape[1]))\n",
        "    \n",
        "                for bbox in bounding_boxes:\n",
        "                    cv2.rectangle(image, (int(bbox[1]*transformed_img_rgb.shape[0]), int(bbox[2]*transformed_img_rgb.shape[1])), (int(bbox[3]*transformed_img_rgb.shape[0]), int(bbox[4]*transformed_img_rgb.shape[1])), (0, 255, 0), 2)\n",
        "                \n",
        "                # Compute the inverse of the homography matrix\n",
        "                inverse_homography = np.linalg.inv(homography)\n",
        "                \n",
        "                transformed_bbox = []\n",
        "                for bbox in enlarge_bbox:    \n",
        "                    # Inverse transform the bounding boxes from the warped perspective to original\n",
        "                    transformed_bbox.append(transform_bounding_box(bbox, inverse_homography,downsampled_image_gray))\n",
        "\n",
        "                for bbox in transformed_bbox:\n",
        "                    # Draw the transformed bounding box on the image\n",
        "                    cv2.rectangle(downsampled_image, (int(bbox[0]), int(bbox[1])),(int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
        "                    cv2.rectangle(downsampled_image, (int(bbox[0]), int(bbox[1])  - 10), (int(bbox[0]) + 100, int(bbox[1])), (0, 255, 0), cv2.FILLED)\n",
        "                    cv2.putText(downsampled_image, \"Non-compliant\", (int(bbox[0]), int(bbox[1])-2), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
        "\n",
        "                predicted_image = Image.fromarray(downsampled_image, 'RGB')\n",
        "                st.image(predicted_image, caption='Processed Image with Detections.', use_column_width=True)\n",
        "\n",
        "def contribute_page():\n",
        "    st.title(\"Contribute\")\n",
        "    st.write(\"Contribute to incremental learning model by providing feedback.\")\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    if 'valid_detections' not in st.session_state:\n",
        "        st.session_state.valid_detections = []\n",
        "\n",
        "    with col1:\n",
        "        uploaded_file = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "      with col2:\n",
        "        # Reset session state for valid_detections on new file upload\n",
        "        st.session_state.valid_detections = []\n",
        "\n",
        "        image = Image.open(uploaded_file).convert('RGB')\n",
        "        image = ImageOps.exif_transpose(image)\n",
        "        results = model.predict(image, conf=0.3)\n",
        "        detections = results[0].boxes  # Detected bounding boxes\n",
        "\n",
        "        # Show the full image with detections in the first column\n",
        "        np_array = results[0].plot()\n",
        "        np_array = cv2.cvtColor(np_array, cv2.COLOR_BGR2RGB)\n",
        "        predicted_image = Image.fromarray(np_array, 'RGB')\n",
        "        with col1:\n",
        "            st.image(predicted_image, caption='Processed Image with Detections.', use_column_width=True)\n",
        "\n",
        "        if detections is not None and len(detections) > 0:\n",
        "            with st.form(key='detections_form'):\n",
        "                for i, box in enumerate(detections):\n",
        "                    bbox = box.xyxy[0].cpu().numpy().tolist()  # Convert tensor to list\n",
        "                    x1, y1, x2, y2 = map(int, bbox)\n",
        "                    margin = 50  # Margin to make the cropped image slightly larger\n",
        "                    x1, y1 = max(0, x1 - margin), max(0, y1 - margin)\n",
        "                    x2, y2 = min(image.width, x2 + margin), min(image.height, y2 + margin)\n",
        "                    cropped_image = image.crop((x1, y1, x2, y2))\n",
        "                    label = int(box.cls.cpu().item())  # Get the class label\n",
        "                    label_name = \"Imprecise\" if label == 0 else \"Missing\"  # Adjust based on your class labels\n",
        "\n",
        "                    col_image, col_radio = st.columns([1, 2])\n",
        "                    with col_image:\n",
        "                        st.image(cropped_image, caption=f\"Detection {i+1}\", width=150)\n",
        "                    with col_radio:\n",
        "                        if st.radio(f\"[{label_name}] Is detection {i+1} correct?\", ('Yes', 'No'), key=f\"radio_{i}\") == 'Yes':\n",
        "                            st.session_state.valid_detections.append({\n",
        "                                \"bbox\": bbox,\n",
        "                                \"class\": int(box.cls.cpu().item())  # Convert tensor to int\n",
        "                            })\n",
        "\n",
        "                submit_button = st.form_submit_button(label='Finish')\n",
        "                if submit_button:\n",
        "                    # After all detections are rated\n",
        "                    annotations = []\n",
        "                    for detection in st.session_state.valid_detections:\n",
        "                        x1, y1, x2, y2 = detection[\"bbox\"]\n",
        "                        label = detection[\"class\"]  # Assuming 'class' is already an int\n",
        "                        x_center = (x1 + x2) / 2 / image.width\n",
        "                        y_center = (y1 + y2) / 2 / image.height\n",
        "                        width = (x2 - x1) / image.width\n",
        "                        height = (y2 - y1) / image.height\n",
        "                        annotations.append(f\"{label} {x_center} {y_center} {width} {height}\")\n",
        "\n",
        "                    # Save the annotated image\n",
        "                    annotated_image_path = os.path.splitext(uploaded_file.name)[0] + \"_annotated.jpg\"\n",
        "                    annotations_path = os.path.splitext(uploaded_file.name)[0] + \".txt\"\n",
        "                    image.save(annotated_image_path)\n",
        "\n",
        "                    with open(annotations_path, 'w') as f:\n",
        "                        f.write(\"\\n\".join(annotations))\n",
        "\n",
        "                    # Upload to Roboflow\n",
        "                    upload_to_roboflow(annotated_image_path, annotations_path, project)\n",
        "\n",
        "                    st.success(\"All detections have been rated and uploaded to Roboflow.\")\n",
        "                    # Reset session state\n",
        "                    st.session_state.valid_detections = []\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"AI Retail Vision\", layout=\"wide\")\n",
        "\n",
        "    tab_names = [\"Detection\", \"Planogram\", \"Contribute\"]\n",
        "    selected_tab = st.sidebar.radio(\"Navigation\", tab_names)\n",
        "\n",
        "    if selected_tab == \"Detection\":\n",
        "        detection_page()\n",
        "    elif selected_tab == \"Planogram\":\n",
        "        planogram_page()\n",
        "    elif selected_tab == \"Contribute\":\n",
        "        contribute_page()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h72tihRyK-3",
        "outputId": "adbc29e4-a4b2-4220-ec4d-b29b5ed4dd49"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "# Kill previous ngrok instances\n",
        "ngrok.kill()\n",
        "ngrok.set_auth_token('2g80v7Wvh6Wrlh2yFO25i3BUpy0_37mg8RF51YAddgyXE3eag')\n",
        "\n",
        "# Start the Streamlit app in the background\n",
        "get_ipython().system_raw('streamlit run --server.port 8501 app.py &')\n",
        "\n",
        "# Set up the ngrok tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(addr=\"8501\", proto='http', bind_tls=True)\n",
        "print(\"Streamlit is running at \", public_url)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
